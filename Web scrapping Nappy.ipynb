{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import  webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tqdm import tqdm\n",
    "path = \"C:\\Program Files (x86)\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  log_to_file(img_url):\n",
    "    with open('.../Documents/links.txt', 'w') as f:\n",
    "        for link in img_url:\n",
    "            f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_scrapper(img_url, browser):\n",
    "    \"\"\"\n",
    "    This function handles the scrapping of images from the website\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        img_url: set containing all images downloaded\n",
    "        browser: instance of chrome with a opened webpage\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "        img_url :set containing updated list\n",
    "    \"\"\"\n",
    "\n",
    "    for x in tqdm(browser.find_elements_by_xpath(\"//div[contains(@data-url, 'https')]\")):\n",
    "        \n",
    "        try:\n",
    "            img = x.get_attribute('data-url')\n",
    "            \n",
    "            \n",
    "            img_url.append(img)\n",
    "            new_filename = img.split('/')[-1]\n",
    "\n",
    "            succounter+= 1\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    time.sleep(30)\n",
    "\n",
    "    return img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_page(succounter, browser, status):\n",
    "    \"\"\"\n",
    "    This function loads the webpage\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "        succounter: counts how many pages scrolled\n",
    "        browser: instance of chrome with a opened webpage\n",
    "    \"\"\"\n",
    "   \n",
    "    browser.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "    more = browser.find_element_by_link_text('Load More')\n",
    "    while True:\n",
    "        try:   \n",
    "            for i in range(10):\n",
    "                browser.execute_script(\"arguments[0].click();\", more)\n",
    "                succounter += 1\n",
    "                print(f'Scrolling to page {succounter}')\n",
    "                time.sleep(20)\n",
    "        except NoSuchElementException as e:\n",
    "            status = False\n",
    "        k = status\n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names1 = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = \"https://www.nappy.co/\"\n",
    "    log_list =  set()\n",
    "\n",
    "\n",
    "    #this options arguments are required for the sake \n",
    "    # of using headless chrome in EC2\n",
    "\n",
    "    options = Options()\n",
    "    driver = webdriver.Chrome(path, chrome_options= options)\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "    succounter = 0\n",
    "\n",
    "    #Extract each name from the file into a set \n",
    "    names = []\n",
    "    status = True\n",
    "\n",
    "    print(\"start scraping ...\")\n",
    "    while status:\n",
    "        status = load_page(succounter, driver, status)\n",
    "        names = image_scrapper(names,driver)\n",
    "    \n",
    "    print(\"Web scraping job completed!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
